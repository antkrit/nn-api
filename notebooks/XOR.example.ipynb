{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import api.lib.autograd as ag\n",
    "\n",
    "from api.lib import loss, activation\n",
    "from api.lib.data import initializers\n",
    "from api.lib.optimizers import GradientDescent"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XOR problem\n",
    "The XOR problem is a classic problem in neural networks researches. The problem is this:\n",
    "given the two inputs (data can only be '0' or '1'), we need to predict the value of XOR function.\n",
    "\n",
    "Here are all the possible inputs and outputs:\n",
    "\n",
    " x | y | out\n",
    "--- |---| ---\n",
    " 0 | 0 | 0\n",
    " 0 | 1 | 1\n",
    " 1 | 0 | 1\n",
    " 1 | 1 | 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0  Loss: 0.3782375055263039\n",
      "Step: 10  Loss: 0.3118054254854621\n",
      "Step: 20  Loss: 0.26874001145773757\n",
      "Step: 30  Loss: 0.2583044942538666\n",
      "Step: 40  Loss: 0.25670670043886407\n",
      "Step: 50  Loss: 0.2564867832178416\n",
      "Step: 60  Loss: 0.25645202872844497\n",
      "Step: 70  Loss: 0.2564390367032165\n",
      "Step: 80  Loss: 0.2564276948598831\n",
      "Step: 90  Loss: 0.25641625110883887\n",
      "Step: 100  Loss: 0.2564048527150168\n",
      "Step: 110  Loss: 0.25639366004405434\n",
      "Step: 120  Loss: 0.2563827406784367\n",
      "Step: 130  Loss: 0.25637211403906435\n",
      "Step: 140  Loss: 0.2563617799225313\n",
      "Step: 150  Loss: 0.25635173083990237\n",
      "Step: 160  Loss: 0.25634195685787375\n",
      "Step: 170  Loss: 0.2563324474298847\n",
      "Step: 180  Loss: 0.25632319207126025\n",
      "Step: 190  Loss: 0.25631418059763245\n",
      "Step: 200  Loss: 0.25630540319977313\n",
      "Step: 210  Loss: 0.2562968504577716\n",
      "Step: 220  Loss: 0.25628851333322905\n",
      "Step: 230  Loss: 0.25628038315395546\n",
      "Step: 240  Loss: 0.2562724515965704\n",
      "Step: 250  Loss: 0.2562647106689864\n",
      "Step: 260  Loss: 0.2562571526934787\n",
      "Step: 270  Loss: 0.2562497702905614\n",
      "Step: 280  Loss: 0.2562425563637139\n",
      "Step: 290  Loss: 0.2562355040849385\n",
      "Step: 300  Loss: 0.25622860688110694\n",
      "Step: 310  Loss: 0.25622185842104933\n",
      "Step: 320  Loss: 0.25621525260333794\n",
      "Step: 330  Loss: 0.25620878354472204\n",
      "Step: 340  Loss: 0.2562024455691703\n",
      "Step: 350  Loss: 0.2561962331974836\n",
      "Step: 360  Loss: 0.25619014113743965\n",
      "Step: 370  Loss: 0.25618416427443647\n",
      "Step: 380  Loss: 0.2561782976626043\n",
      "Step: 390  Loss: 0.2561725365163541\n",
      "Step: 400  Loss: 0.2561668762023373\n",
      "Step: 410  Loss: 0.2561613122317907\n",
      "Step: 420  Loss: 0.25615584025324256\n",
      "Step: 430  Loss: 0.2561504560455575\n",
      "Step: 440  Loss: 0.25614515551130124\n",
      "Step: 450  Loss: 0.25613993467040314\n",
      "Step: 460  Loss: 0.25613478965410075\n",
      "Step: 470  Loss: 0.2561297166991487\n",
      "Step: 480  Loss: 0.2561247121422764\n",
      "Step: 490  Loss: 0.25611977241487893\n",
      "Step: 500  Loss: 0.2561148940379292\n",
      "Step: 510  Loss: 0.2561100736170958\n",
      "Step: 520  Loss: 0.2561053078380572\n",
      "Step: 530  Loss: 0.25610059346199904\n",
      "Step: 540  Loss: 0.25609592732128394\n",
      "Step: 550  Loss: 0.25609130631528515\n",
      "Step: 560  Loss: 0.2560867274063725\n",
      "Step: 570  Loss: 0.25608218761604384\n",
      "Step: 580  Loss: 0.25607768402119235\n",
      "Step: 590  Loss: 0.2560732137505015\n",
      "Step: 600  Loss: 0.25606877398096184\n",
      "Step: 610  Loss: 0.2560643619345005\n",
      "Step: 620  Loss: 0.2560599748747182\n",
      "Step: 630  Loss: 0.2560556101037273\n",
      "Step: 640  Loss: 0.25605126495908337\n",
      "Step: 650  Loss: 0.25604693681080726\n",
      "Step: 660  Loss: 0.25604262305848924\n",
      "Step: 670  Loss: 0.2560383211284726\n",
      "Step: 680  Loss: 0.25603402847111023\n",
      "Step: 690  Loss: 0.2560297425580892\n",
      "Step: 700  Loss: 0.25602546087982053\n",
      "Step: 710  Loss: 0.2560211809428876\n",
      "Step: 720  Loss: 0.2560169002675507\n",
      "Step: 730  Loss: 0.2560126163853022\n",
      "Step: 740  Loss: 0.2560083268364696\n",
      "Step: 750  Loss: 0.2560040291678612\n",
      "Step: 760  Loss: 0.255999720930452\n",
      "Step: 770  Loss: 0.25599539967710505\n",
      "Step: 780  Loss: 0.255991062960325\n",
      "Step: 790  Loss: 0.2559867083300396\n",
      "Step: 800  Loss: 0.25598233333140663\n",
      "Step: 810  Loss: 0.25597793550264103\n",
      "Step: 820  Loss: 0.2559735123728602\n",
      "Step: 830  Loss: 0.2559690614599425\n",
      "Step: 840  Loss: 0.2559645802683959\n",
      "Step: 850  Loss: 0.2559600662872328\n",
      "Step: 860  Loss: 0.25595551698784635\n",
      "Step: 870  Loss: 0.2559509298218866\n",
      "Step: 880  Loss: 0.2559463022191293\n",
      "Step: 890  Loss: 0.255941631585335\n",
      "Step: 900  Loss: 0.2559369153000949\n",
      "Step: 910  Loss: 0.2559321507146557\n",
      "Step: 920  Loss: 0.255927335149723\n",
      "Step: 930  Loss: 0.2559224658932345\n",
      "Step: 940  Loss: 0.25591754019810076\n",
      "Step: 950  Loss: 0.25591255527990586\n",
      "Step: 960  Loss: 0.25590750831456494\n",
      "Step: 970  Loss: 0.25590239643592955\n",
      "Step: 980  Loss: 0.25589721673333776\n",
      "Step: 990  Loss: 0.2558919662491005\n",
      "Step: 1000  Loss: 0.2558866419759178\n",
      "Step: 1010  Loss: 0.2558812408542189\n",
      "Step: 1020  Loss: 0.2558757597694158\n",
      "Step: 1030  Loss: 0.255870195549066\n",
      "Step: 1040  Loss: 0.25586454495993266\n",
      "Step: 1050  Loss: 0.2558588047049361\n",
      "Step: 1060  Loss: 0.25585297141998486\n",
      "Step: 1070  Loss: 0.25584704167067773\n",
      "Step: 1080  Loss: 0.255841011948867\n",
      "Step: 1090  Loss: 0.2558348786690704\n",
      "Step: 1100  Loss: 0.2558286381647211\n",
      "Step: 1110  Loss: 0.2558222866842438\n",
      "Step: 1120  Loss: 0.25581582038694367\n",
      "Step: 1130  Loss: 0.25580923533869476\n",
      "Step: 1140  Loss: 0.2558025275074138\n",
      "Step: 1150  Loss: 0.2557956927583042\n",
      "Step: 1160  Loss: 0.2557887268488559\n",
      "Step: 1170  Loss: 0.2557816254235814\n",
      "Step: 1180  Loss: 0.2557743840084748\n",
      "Step: 1190  Loss: 0.25576699800517255\n",
      "Step: 1200  Loss: 0.2557594626847977\n",
      "Step: 1210  Loss: 0.25575177318146813\n",
      "Step: 1220  Loss: 0.2557439244854472\n",
      "Step: 1230  Loss: 0.25573591143591395\n",
      "Step: 1240  Loss: 0.25572772871333205\n",
      "Step: 1250  Loss: 0.2557193708313904\n",
      "Step: 1260  Loss: 0.25571083212849127\n",
      "Step: 1270  Loss: 0.2557021067587604\n",
      "Step: 1280  Loss: 0.25569318868254814\n",
      "Step: 1290  Loss: 0.2556840716563964\n",
      "Step: 1300  Loss: 0.2556747492224363\n",
      "Step: 1310  Loss: 0.2556652146971892\n",
      "Step: 1320  Loss: 0.2556554611597331\n",
      "Step: 1330  Loss: 0.2556454814392025\n",
      "Step: 1340  Loss: 0.2556352681015845\n",
      "Step: 1350  Loss: 0.25562481343577037\n",
      "Step: 1360  Loss: 0.2556141094388268\n",
      "Step: 1370  Loss: 0.25560314780044013\n",
      "Step: 1380  Loss: 0.25559191988649343\n",
      "Step: 1390  Loss: 0.2555804167217285\n",
      "Step: 1400  Loss: 0.25556862897144583\n",
      "Step: 1410  Loss: 0.2555565469221916\n",
      "Step: 1420  Loss: 0.25554416046138\n",
      "Step: 1430  Loss: 0.25553145905579666\n",
      "Step: 1440  Loss: 0.2555184317289238\n",
      "Step: 1450  Loss: 0.2555050670370305\n",
      "Step: 1460  Loss: 0.25549135304396264\n",
      "Step: 1470  Loss: 0.25547727729457004\n",
      "Step: 1480  Loss: 0.25546282678669974\n",
      "Step: 1490  Loss: 0.25544798794168744\n",
      "Step: 1500  Loss: 0.2554327465732702\n",
      "Step: 1510  Loss: 0.2554170878548436\n",
      "Step: 1520  Loss: 0.25540099628498436\n",
      "Step: 1530  Loss: 0.2553844556511495\n",
      "Step: 1540  Loss: 0.2553674489914693\n",
      "Step: 1550  Loss: 0.25534995855453746\n",
      "Step: 1560  Loss: 0.2553319657571055\n",
      "Step: 1570  Loss: 0.25531345113958026\n",
      "Step: 1580  Loss: 0.2552943943192205\n",
      "Step: 1590  Loss: 0.25527477394092507\n",
      "Step: 1600  Loss: 0.25525456762549825\n",
      "Step: 1610  Loss: 0.255233751915277\n",
      "Step: 1620  Loss: 0.2552123022169963\n",
      "Step: 1630  Loss: 0.255190192741768\n",
      "Step: 1640  Loss: 0.2551673964420388\n",
      "Step: 1650  Loss: 0.2551438849453944\n",
      "Step: 1660  Loss: 0.2551196284850668\n",
      "Step: 1670  Loss: 0.25509459582699795\n",
      "Step: 1680  Loss: 0.25506875419331\n",
      "Step: 1690  Loss: 0.2550420691820249\n",
      "Step: 1700  Loss: 0.2550145046828725\n",
      "Step: 1710  Loss: 0.25498602278902055\n",
      "Step: 1720  Loss: 0.25495658370455493\n",
      "Step: 1730  Loss: 0.2549261456475346\n",
      "Step: 1740  Loss: 0.25489466474843936\n",
      "Step: 1750  Loss: 0.25486209494382484\n",
      "Step: 1760  Loss: 0.25482838786499334\n",
      "Step: 1770  Loss: 0.25479349272148616\n",
      "Step: 1780  Loss: 0.2547573561791962\n",
      "Step: 1790  Loss: 0.25471992223289663\n",
      "Step: 1800  Loss: 0.25468113207297804\n",
      "Step: 1810  Loss: 0.25464092394617865\n",
      "Step: 1820  Loss: 0.2545992330100913\n",
      "Step: 1830  Loss: 0.2545559911812254\n",
      "Step: 1840  Loss: 0.25451112697639605\n",
      "Step: 1850  Loss: 0.25446456534721057\n",
      "Step: 1860  Loss: 0.2544162275074162\n",
      "Step: 1870  Loss: 0.25436603075286646\n",
      "Step: 1880  Loss: 0.25431388827386114\n",
      "Step: 1890  Loss: 0.2542597089596058\n",
      "Step: 1900  Loss: 0.2542033971945293\n",
      "Step: 1910  Loss: 0.25414485264619224\n",
      "Step: 1920  Loss: 0.2540839700445069\n",
      "Step: 1930  Loss: 0.2540206389519812\n",
      "Step: 1940  Loss: 0.2539547435246827\n",
      "Step: 1950  Loss: 0.2538861622636085\n",
      "Step: 1960  Loss: 0.2538147677561262\n",
      "Step: 1970  Loss: 0.25374042640713484\n",
      "Step: 1980  Loss: 0.25366299815956983\n",
      "Step: 1990  Loss: 0.25358233620385306\n",
      "Step: 2000  Loss: 0.25349828667585833\n",
      "Step: 2010  Loss: 0.25341068834293456\n",
      "Step: 2020  Loss: 0.2533193722774908\n",
      "Step: 2030  Loss: 0.2532241615176106\n",
      "Step: 2040  Loss: 0.25312487071412415\n",
      "Step: 2050  Loss: 0.2530213057635206\n",
      "Step: 2060  Loss: 0.2529132634260414\n",
      "Step: 2070  Loss: 0.25280053092824967\n",
      "Step: 2080  Loss: 0.25268288554932544\n",
      "Step: 2090  Loss: 0.2525600941903004\n",
      "Step: 2100  Loss: 0.2524319129254034\n",
      "Step: 2110  Loss: 0.25229808653466834\n",
      "Step: 2120  Loss: 0.25215834801693715\n",
      "Step: 2130  Loss: 0.2520124180823904\n",
      "Step: 2140  Loss: 0.2518600046237676\n",
      "Step: 2150  Loss: 0.2517008021654821\n",
      "Step: 2160  Loss: 0.25153449128992644\n",
      "Step: 2170  Loss: 0.2513607380403909\n",
      "Step: 2180  Loss: 0.2511791933001963\n",
      "Step: 2190  Loss: 0.2509894921478861\n",
      "Step: 2200  Loss: 0.2507912531886386\n",
      "Step: 2210  Loss: 0.25058407786245995\n",
      "Step: 2220  Loss: 0.25036754973022124\n",
      "Step: 2230  Loss: 0.25014123373921493\n",
      "Step: 2240  Loss: 0.2499046754706528\n",
      "Step: 2250  Loss: 0.2496574003724098\n",
      "Step: 2260  Loss: 0.24939891298136974\n",
      "Step: 2270  Loss: 0.24912869614094701\n",
      "Step: 2280  Loss: 0.24884621022077175\n",
      "Step: 2290  Loss: 0.2485508923471372\n",
      "Step: 2300  Loss: 0.2482421556546322\n",
      "Step: 2310  Loss: 0.24791938857142076\n",
      "Step: 2320  Loss: 0.24758195415288758\n",
      "Step: 2330  Loss: 0.24722918948083383\n",
      "Step: 2340  Loss: 0.24686040514806412\n",
      "Step: 2350  Loss: 0.2464748848510238\n",
      "Step: 2360  Loss: 0.2460718851160873\n",
      "Step: 2370  Loss: 0.24565063518808883\n",
      "Step: 2380  Loss: 0.24521033711265927\n",
      "Step: 2390  Loss: 0.24475016604676503\n",
      "Step: 2400  Loss: 0.24426927083441052\n",
      "Step: 2410  Loss: 0.24376677488659632\n",
      "Step: 2420  Loss: 0.24324177740611774\n",
      "Step: 2430  Loss: 0.24269335499842276\n",
      "Step: 2440  Loss: 0.24212056370924645\n",
      "Step: 2450  Loss: 0.24152244152782645\n",
      "Step: 2460  Loss: 0.24089801139086292\n",
      "Step: 2470  Loss: 0.24024628471670037\n",
      "Step: 2480  Loss: 0.23956626549118265\n",
      "Step: 2490  Loss: 0.2388569549159788\n",
      "Step: 2500  Loss: 0.23811735661670566\n",
      "Step: 2510  Loss: 0.23734648239174977\n",
      "Step: 2520  Loss: 0.236543358463353\n",
      "Step: 2530  Loss: 0.23570703217045705\n",
      "Step: 2540  Loss: 0.2348365790184041\n",
      "Step: 2550  Loss: 0.23393110997451766\n",
      "Step: 2560  Loss: 0.23298977887173278\n",
      "Step: 2570  Loss: 0.2320117897559859\n",
      "Step: 2580  Loss: 0.23099640398841448\n",
      "Step: 2590  Loss: 0.2299429468921599\n",
      "Step: 2600  Loss: 0.22885081371742336\n",
      "Step: 2610  Loss: 0.22771947468911508\n",
      "Step: 2620  Loss: 0.22654847890055335\n",
      "Step: 2630  Loss: 0.2253374568255263\n",
      "Step: 2640  Loss: 0.2240861212405465\n",
      "Step: 2650  Loss: 0.22279426637965408\n",
      "Step: 2660  Loss: 0.2214617651854131\n",
      "Step: 2670  Loss: 0.2200885645708544\n",
      "Step: 2680  Loss: 0.2186746786664761\n",
      "Step: 2690  Loss: 0.21722018009188324\n",
      "Step: 2700  Loss: 0.21572518936068696\n",
      "Step: 2710  Loss: 0.2141898625971162\n",
      "Step: 2720  Loss: 0.21261437781063844\n",
      "Step: 2730  Loss: 0.21099892003814089\n",
      "Step: 2740  Loss: 0.2093436657196498\n",
      "Step: 2750  Loss: 0.20764876672141164\n",
      "Step: 2760  Loss: 0.20591433445820612\n",
      "Step: 2770  Loss: 0.20414042459434356\n",
      "Step: 2780  Loss: 0.20232702281969941\n",
      "Step: 2790  Loss: 0.20047403220352494\n",
      "Step: 2800  Loss: 0.19858126262502063\n",
      "Step: 2810  Loss: 0.1966484227662279\n",
      "Step: 2820  Loss: 0.19467511513008023\n",
      "Step: 2830  Loss: 0.1926608345146702\n",
      "Step: 2840  Loss: 0.19060497033390045\n",
      "Step: 2850  Loss: 0.18850681312432688\n",
      "Step: 2860  Loss: 0.18636556551751976\n",
      "Step: 2870  Loss: 0.18418035788571974\n",
      "Step: 2880  Loss: 0.18195026878484935\n",
      "Step: 2890  Loss: 0.17967435022188327\n",
      "Step: 2900  Loss: 0.17735165766218455\n",
      "Step: 2910  Loss: 0.17498128456602458\n",
      "Step: 2920  Loss: 0.17256240110211715\n",
      "Step: 2930  Loss: 0.17009429653056218\n",
      "Step: 2940  Loss: 0.16757642458028113\n",
      "Step: 2950  Loss: 0.16500845097058525\n",
      "Step: 2960  Loss: 0.16239030204846105\n",
      "Step: 2970  Loss: 0.159722213339982\n",
      "Step: 2980  Loss: 0.15700477665539891\n",
      "Step: 2990  Loss: 0.15423898425418184\n",
      "Step: 3000  Loss: 0.15142626848118534\n",
      "Step: 3010  Loss: 0.148568535241481\n",
      "Step: 3020  Loss: 0.14566818970214834\n",
      "Step: 3030  Loss: 0.14272815270571804\n",
      "Step: 3040  Loss: 0.139751866560148\n",
      "Step: 3050  Loss: 0.1367432891376023\n",
      "Step: 3060  Loss: 0.13370687556617766\n",
      "Step: 3070  Loss: 0.13064754722519606\n",
      "Step: 3080  Loss: 0.12757064823823977\n",
      "Step: 3090  Loss: 0.12448189017402865\n",
      "Step: 3100  Loss: 0.12138728618299711\n",
      "Step: 3110  Loss: 0.11829307628292843\n",
      "Step: 3120  Loss: 0.11520564592572782\n",
      "Step: 3130  Loss: 0.11213144029784552\n",
      "Step: 3140  Loss: 0.10907687700395544\n",
      "Step: 3150  Loss: 0.10604825984156285\n",
      "Step: 3160  Loss: 0.10305169628880478\n",
      "Step: 3170  Loss: 0.10009302110621249\n",
      "Step: 3180  Loss: 0.09717772811399761\n",
      "Step: 3190  Loss: 0.09431091177677517\n",
      "Step: 3200  Loss: 0.09149721974060837\n",
      "Step: 3210  Loss: 0.0887408169581781\n",
      "Step: 3220  Loss: 0.08604536154076767\n",
      "Step: 3230  Loss: 0.08341399202030562\n",
      "Step: 3240  Loss: 0.08084932531405123\n",
      "Step: 3250  Loss: 0.07835346437398676\n",
      "Step: 3260  Loss: 0.07592801427993584\n",
      "Step: 3270  Loss: 0.07357410539982764\n",
      "Step: 3280  Loss: 0.07129242218625326\n",
      "Step: 3290  Loss: 0.06908323619492485\n",
      "Step: 3300  Loss: 0.06694644198452598\n",
      "Step: 3310  Loss: 0.06488159467428932\n",
      "Step: 3320  Loss: 0.06288794808123395\n",
      "Step: 3330  Loss: 0.06096449252036992\n",
      "Step: 3340  Loss: 0.05910999151725549\n",
      "Step: 3350  Loss: 0.05732301684422117\n",
      "Step: 3360  Loss: 0.055601981442810464\n",
      "Step: 3370  Loss: 0.053945169931102474\n",
      "Step: 3380  Loss: 0.052350766513069004\n",
      "Step: 3390  Loss: 0.05081688020698154\n",
      "Step: 3400  Loss: 0.04934156739134399\n",
      "Step: 3410  Loss: 0.04792285173093723\n",
      "Step: 3420  Loss: 0.04655874159396035\n",
      "Step: 3430  Loss: 0.04524724510585761\n",
      "Step: 3440  Loss: 0.043986383008298574\n",
      "Step: 3450  Loss: 0.04277419950494547\n",
      "Step: 3460  Loss: 0.04160877128100807\n",
      "Step: 3470  Loss: 0.04048821488287603\n",
      "Step: 3480  Loss: 0.039410692638831644\n",
      "Step: 3490  Loss: 0.03837441729327655\n",
      "Step: 3500  Loss: 0.037377655516109914\n",
      "Step: 3510  Loss: 0.03641873043674578\n",
      "Step: 3520  Loss: 0.03549602333942162\n",
      "Step: 3530  Loss: 0.034607974643467834\n",
      "Step: 3540  Loss: 0.03375308427945378\n",
      "Step: 3550  Loss: 0.03292991155988577\n",
      "Step: 3560  Loss: 0.032137074631595625\n",
      "Step: 3570  Loss: 0.03137324958623443\n",
      "Step: 3580  Loss: 0.03063716929544995\n",
      "Step: 3590  Loss: 0.02992762202838068\n",
      "Step: 3600  Loss: 0.029243449901054548\n",
      "Step: 3610  Loss: 0.028583547200082\n",
      "Step: 3620  Loss: 0.027946858616655676\n",
      "Step: 3630  Loss: 0.02733237742123763\n",
      "Step: 3640  Loss: 0.026739143604384584\n",
      "Step: 3650  Loss: 0.026166242004857586\n",
      "Step: 3660  Loss: 0.02561280044243118\n",
      "Step: 3670  Loss: 0.02507798786959519\n",
      "Step: 3680  Loss: 0.024561012553571838\n",
      "Step: 3690  Loss: 0.0240611202977046\n",
      "Step: 3700  Loss: 0.023577592709258542\n",
      "Step: 3710  Loss: 0.023109745518964893\n",
      "Step: 3720  Loss: 0.02265692695620542\n",
      "Step: 3730  Loss: 0.022218516182525964\n",
      "Step: 3740  Loss: 0.021793921785166114\n",
      "Step: 3750  Loss: 0.021382580331462394\n",
      "Step: 3760  Loss: 0.02098395498430085\n",
      "Step: 3770  Loss: 0.020597534178242824\n",
      "Step: 3780  Loss: 0.02022283035550237\n",
      "Step: 3790  Loss: 0.019859378760600514\n",
      "Step: 3800  Loss: 0.0195067362922483\n",
      "Step: 3810  Loss: 0.01916448041079895\n",
      "Step: 3820  Loss: 0.018832208099457272\n",
      "Step: 3830  Loss: 0.018509534877324214\n",
      "Step: 3840  Loss: 0.018196093862284393\n",
      "Step: 3850  Loss: 0.0178915348817051\n",
      "Step: 3860  Loss: 0.01759552362890075\n",
      "Step: 3870  Loss: 0.01730774086332514\n",
      "Step: 3880  Loss: 0.017027881652475423\n",
      "Step: 3890  Loss: 0.016755654653531016\n",
      "Step: 3900  Loss: 0.016490781432795382\n",
      "Step: 3910  Loss: 0.01623299582106563\n",
      "Step: 3920  Loss: 0.015982043303114455\n",
      "Step: 3930  Loss: 0.015737680439534302\n",
      "Step: 3940  Loss: 0.015499674319262025\n",
      "Step: 3950  Loss: 0.015267802041169768\n",
      "Step: 3960  Loss: 0.015041850223180428\n",
      "Step: 3970  Loss: 0.01482161453743296\n",
      "Step: 3980  Loss: 0.014606899270094524\n",
      "Step: 3990  Loss: 0.014397516904481818\n",
      "Step: 4000  Loss: 0.014193287726222716\n",
      "Step: 4010  Loss: 0.013994039449251384\n",
      "Step: 4020  Loss: 0.013799606861494314\n",
      "Step: 4030  Loss: 0.013609831489163096\n",
      "Step: 4040  Loss: 0.013424561278628776\n",
      "Step: 4050  Loss: 0.013243650294907192\n",
      "Step: 4060  Loss: 0.013066958435837602\n",
      "Step: 4070  Loss: 0.012894351161087468\n",
      "Step: 4080  Loss: 0.012725699235164756\n",
      "Step: 4090  Loss: 0.012560878483663631\n",
      "Step: 4100  Loss: 0.012399769562013843\n",
      "Step: 4110  Loss: 0.012242257736044985\n",
      "Step: 4120  Loss: 0.012088232673715355\n",
      "Step: 4130  Loss: 0.011937588247392264\n",
      "Step: 4140  Loss: 0.011790222346105814\n",
      "Step: 4150  Loss: 0.011646036697230607\n",
      "Step: 4160  Loss: 0.011504936697081126\n",
      "Step: 4170  Loss: 0.011366831249936166\n",
      "Step: 4180  Loss: 0.01123163261503478\n",
      "Step: 4190  Loss: 0.011099256261113084\n",
      "Step: 4200  Loss: 0.010969620728075419\n",
      "Step: 4210  Loss: 0.01084264749541617\n",
      "Step: 4220  Loss: 0.010718260857031654\n",
      "Step: 4230  Loss: 0.010596387802080517\n",
      "Step: 4240  Loss: 0.010476957901571717\n",
      "Step: 4250  Loss: 0.010359903200376682\n",
      "Step: 4260  Loss: 0.010245158114379715\n",
      "Step: 4270  Loss: 0.010132659332496661\n",
      "Step: 4280  Loss: 0.01002234572330749\n",
      "Step: 4290  Loss: 0.009914158246062205\n",
      "Step: 4300  Loss: 0.009808039865833512\n",
      "Step: 4310  Loss: 0.009703935472601968\n",
      "Step: 4320  Loss: 0.009601791804071376\n",
      "Step: 4330  Loss: 0.009501557372023763\n",
      "Step: 4340  Loss: 0.009403182392033195\n",
      "Step: 4350  Loss: 0.009306618716368537\n",
      "Step: 4360  Loss: 0.009211819769923432\n",
      "Step: 4370  Loss: 0.009118740489022425\n",
      "Step: 4380  Loss: 0.009027337262957933\n",
      "Step: 4390  Loss: 0.008937567878123522\n",
      "Step: 4400  Loss: 0.008849391464613731\n",
      "Step: 4410  Loss: 0.008762768445169385\n",
      "Step: 4420  Loss: 0.008677660486352998\n",
      "Step: 4430  Loss: 0.008594030451845572\n",
      "Step: 4440  Loss: 0.00851184235776124\n",
      "Step: 4450  Loss: 0.0084310613298825\n",
      "Step: 4460  Loss: 0.008351653562723518\n",
      "Step: 4470  Loss: 0.00827358628033363\n",
      "Step: 4480  Loss: 0.00819682769875834\n",
      "Step: 4490  Loss: 0.008121346990079138\n",
      "Step: 4500  Loss: 0.008047114247957506\n",
      "Step: 4510  Loss: 0.007974100454612392\n",
      "Step: 4520  Loss: 0.007902277449164316\n",
      "Step: 4530  Loss: 0.00783161789728242\n",
      "Step: 4540  Loss: 0.007762095262073941\n",
      "Step: 4550  Loss: 0.00769368377615946\n",
      "Step: 4560  Loss: 0.00762635841487866\n",
      "Step: 4570  Loss: 0.007560094870576061\n",
      "Step: 4580  Loss: 0.0074948695279168686\n",
      "Step: 4590  Loss: 0.007430659440186802\n",
      "Step: 4600  Loss: 0.007367442306531667\n",
      "Step: 4610  Loss: 0.0073051964500946\n",
      "Step: 4620  Loss: 0.007243900797010932\n",
      "Step: 4630  Loss: 0.007183534856222825\n",
      "Step: 4640  Loss: 0.0071240787000776875\n",
      "Step: 4650  Loss: 0.00706551294567541\n",
      "Step: 4660  Loss: 0.0070078187369327655\n",
      "Step: 4670  Loss: 0.006950977727332693\n",
      "Step: 4680  Loss: 0.006894972063329731\n",
      "Step: 4690  Loss: 0.006839784368382976\n",
      "Step: 4700  Loss: 0.006785397727589819\n",
      "Step: 4710  Loss: 0.006731795672894867\n",
      "Step: 4720  Loss: 0.0066789621688497875\n",
      "Step: 4730  Loss: 0.006626881598900506\n",
      "Step: 4740  Loss: 0.006575538752180192\n",
      "Step: 4750  Loss: 0.006524918810786274\n",
      "Step: 4760  Loss: 0.006475007337521875\n",
      "Step: 4770  Loss: 0.006425790264082275\n",
      "Step: 4780  Loss: 0.006377253879667859\n",
      "Step: 4790  Loss: 0.006329384820006639\n",
      "Step: 4800  Loss: 0.006282170056768919\n",
      "Step: 4810  Loss: 0.006235596887358969\n",
      "Step: 4820  Loss: 0.006189652925067787\n",
      "Step: 4830  Loss: 0.006144326089572956\n",
      "Step: 4840  Loss: 0.006099604597771443\n",
      "Step: 4850  Loss: 0.006055476954932092\n",
      "Step: 4860  Loss: 0.006011931946155338\n",
      "Step: 4870  Loss: 0.005968958628127728\n",
      "Step: 4880  Loss: 0.005926546321160049\n",
      "Step: 4890  Loss: 0.005884684601497599\n",
      "Step: 4900  Loss: 0.0058433632938922455\n",
      "Step: 4910  Loss: 0.005802572464426221\n",
      "Step: 4920  Loss: 0.0057623024135776526\n",
      "Step: 4930  Loss: 0.005722543669518813\n",
      "Step: 4940  Loss: 0.005683286981638163\n",
      "Step: 4950  Loss: 0.005644523314277634\n",
      "Step: 4960  Loss: 0.005606243840676983\n",
      "Step: 4970  Loss: 0.0055684399371175395\n",
      "Step: 4980  Loss: 0.005531103177257742\n",
      "Step: 4990  Loss: 0.0054942253266534215\n",
      "Step: 5000  Loss: 0.00545779833745591\n"
     ]
    }
   ],
   "source": [
    "X = ag.Placeholder('x')\n",
    "y = ag.Placeholder('y')\n",
    "\n",
    "session = ag.Session()\n",
    "factv = activation.Sigmoid(session=session)\n",
    "floss = loss.MSE(session)\n",
    "\n",
    "# Create a hidden layer with 3 nodes\n",
    "W_hidden = initializers.random_uniform(2, 2)\n",
    "b_hidden = initializers.random_uniform(1, 2)\n",
    "p_hidden = factv(X @ W_hidden + b_hidden)\n",
    "\n",
    "# Create the output layer\n",
    "W_output = initializers.random_uniform(2, 1)\n",
    "b_output = initializers.random_uniform(1, 1)\n",
    "p_output = factv(p_hidden @ W_output + b_output)\n",
    "\n",
    "# Calculate mse loss\n",
    "J = floss(p_output, y)\n",
    "\n",
    "trainable = [W_hidden, b_hidden, W_output, b_output]\n",
    "optimizer = GradientDescent(lr=0.1, trainable_variables=trainable)\n",
    "\n",
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "err_history = []\n",
    "epoch = 5000\n",
    "samples = len(x_train)\n",
    "\n",
    "for step in range(epoch+1):\n",
    "    err = 0\n",
    "\n",
    "    for j in range(samples):\n",
    "        feed_dict = {\n",
    "            'x': x_train[j],\n",
    "            'y': y_train[j]\n",
    "        }\n",
    "        err += session.run(J, feed_dict)\n",
    "        optimizer.minimize(J)\n",
    "\n",
    "    err_history.append(err / samples)\n",
    "    if step % 10 == 0:\n",
    "        print(\"Step:\", step, \" Loss:\", err / samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]: [0.07660171]\n",
      "[0 1]: [0.92903406]\n",
      "[1 0]: [0.92888911]\n",
      "[1 1]: [0.07611333]\n"
     ]
    }
   ],
   "source": [
    "for i in range(samples):\n",
    "    output = session.run(p_output, {'x': x_train[i]})\n",
    "    print(f\"{x_train[i][0]}: {output[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(0, epoch+1), err_history)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}